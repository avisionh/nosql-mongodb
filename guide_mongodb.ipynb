{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚òïÔ∏èüë©üèΩ‚Äçüíª NoSQL, MongoDB and Python üêç\n",
    "This Jupyter notebook establishes the following lines of questioning:\n",
    "1. What is No-SQL, especially with reference to SQL databases?\n",
    "1. Why would you use a NoSQL database over SQL databases?\n",
    "1. How can you setup a (*free*) cloud-based No-SQL database?\n",
    "1. Other vendors that you can set-up NoSQL databases with, and their difference to [MongoDB](https://www.mongodb.com/), the choice used here.\n",
    "1. How you can interact with your NoSQL database in the form of?\n",
    "    + Connecting to the database\n",
    "    + Viewing *objects*\n",
    "    + Importing data\n",
    "    \n",
    "    \n",
    "For this session, are accessing a cluster already set-up. Accessing this cluster and performing the tasks outlined above would require **READ-WRITE** access. To obtain the ability to interact with the cluster used in this notebook, please contact [Avision Ho](https://github.com/avisionh) or set-up your own and amend the details. ü§°\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get location of Python installation\n",
    "import sys\n",
    "import os\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "Start by establishing the base for our code later on.\n",
    "\n",
    "Note, that to effectively run this notebook through your organisation's proxy, including getting round issues of `pip install pymongo --proxy <ip_address>.<port_number>`, then will use a Google's hosted [Jupyter Notebook service](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install pymongo\n",
    "!pip install kaggle;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount GDrive to GColab - only needs to be done once!\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's connect to the Kaggle API\n",
    "As part of our experimentation, will be downloading data directly from Kaggle via their API. To do so, will need to follow their guidance [here](https://github.com/Kaggle/kaggle-api).\n",
    "\n",
    "In particular, will need a API token and have this saved in our Google Colab session so that it can access the Kaggle API. Guidance on doing this is [here](https://stackoverflow.com/questions/49310470/using-kaggle-datasets-in-google-colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upload kaggle.json file - only needs to be done once!\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move into folder which Kaggle API client expects - only needs to be done once!\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For security, hide API key - only needs to be done once!\n",
    "# Make API key hidden\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to load packages\n",
    "Next step is to load our packages so we can use functions within them later in our notebook. Also, check our working directory for interest and authenticate the **Kaggle API client** so we can download data from the web servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant packages\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import kaggle\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check current working directory\n",
    "path_folder = os.getcwd()\n",
    "print(path_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload user_credentials file for connecting to MongoDB cluster - only needs to be done once!\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate Kaggle API to enable downloading data from there\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory and Concepts\n",
    "Here, we will discuss what NoSQL is, why you will use it, especially with reference to SQL, and important pieces of language to pick-up when talking NoSQL.\n",
    "\n",
    "For further reading, please visit:\n",
    "- [mongoDB website](https://www.mongodb.com/nosql-inline)\n",
    "- [panoply blog](https://blog.panoply.io/sql-or-nosql-that-is-the-question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this new-fangled NoSQL then? ü§∑üèª‚Äç‚ôÄÔ∏è\n",
    "NoSQL is **not** SQL.\n",
    "\n",
    "NoSQL databases are non-relational databases, whereas SQL databases are relational databases.\n",
    "\n",
    "Whereas data is stored in a tabular format within SQL databases, this is eschewed in NoSQL databases where the data can be stored in various forms, and in most cases without any structure at all!\n",
    "\n",
    "These databases are designed to handle the processing of dynamically evolving, real-time changing large-scale and unstructured data.\n",
    "\n",
    "Essentially, NoSQL databases are built to be flexible and scalable so that they can hold any type of data at large volumes and with little work in creating a pre-existing structure to hold the data. üßò‚Äç‚ôÇÔ∏èü§∏‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Due to these features, they are ideal of Agile development methods to iteratively building software.\n",
    "\n",
    "There are several implementations of NoSQL databases, which are categoried in the following families:\n",
    "1. **Key-value stores** | Every item in the database is stored as a *key* alongside its *value*, as *key-value* pairs. These are the simplest NoSQL databases. Popular implementation is [dynamoDB](https://aws.amazon.com/dynamodb/).\n",
    "1. **Document databases** | These pair each *key* with a complext data structure known as a *document*. *Documents* can contain many different *key-value* pairs, or key-array pairs, or even nested documents. Can be thought of as an extension, higher-functionality version of **key-value stores**. Popular implementation is [mongoDB](https://www.mongodb.com/).\n",
    "1. **Wide-column stores** | Designed to store columns of data together, rather than rows. Popular implementation is [Apache Cassandra](http://cassandra.apache.org/).\n",
    "1. **Graph stores** | Store information about networks that connections between objects. The most obvious example is provided by social networks. Popular implementation is [Neo4j](https://neo4j.com/).\n",
    "\n",
    "![ConfusedUrl](https://media.giphy.com/media/a0FuPjiLZev4c/giphy.gif \"confused\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should I use NoSQL over my dearly beloved SQL? üë¥üèΩ\n",
    "Answer is that it depends on **business context**.\n",
    "\n",
    "Use SQL when:\n",
    "- You can define your data structure upfront and it is unlikely to change\n",
    "- You want strong data integrity/quality in terms of its accuracy and completeness/comprehensiveness.\n",
    "- You need to perform complex queries\n",
    "\n",
    "Use NoSQL when:\n",
    "- Your data is not in tabular format *e.g. hierarchical, graph network*\n",
    "- Your data structure can change over time\n",
    "- You anticipate storing large reams of data, so desire scalability\n",
    "- You anticipate evolving and changing data requirements over time, so desire flexibility\n",
    "\n",
    "#### üìö Case Study - Amaze-on üìö\n",
    "As an ecommerce giant, you sell a phenomenal range of products (*e.g. books, electronics, clothes and even food*), with each of these products having a number of characteristics associated with them (*e.g. price, weight, dimensions, manufacturer description, reviews*). üç£ü•°ü•¢\n",
    "\n",
    "In theory, these products, their product categories and characteristics could all be stored in a relational database such as SQL. However, if a new characteristic such as user reviews were to be added, it may require that the entire database is destroyed and re-designed to incorporate this. With a non-relational database, there is no need to start from scratch - **flexibility**. ‚úîÔ∏è\n",
    "\n",
    "Given how many orders you collect and process (not to mention that you also monitor customers' activity in browsing and mulling over products), then you are collecting huge reams of data. There will come a point where you have reached their 'computer memory' limit. Under a relational database system which is **vertically-scalable**, this means to increase your 'computer memory', you will need to buy a bigger computer with more storage space, and transfer all that data over - getting rid of the old one. Whereas with a non-relational database system which is **horizontally-scalable**, you only need to buy an extra computer with less memory, add it to the network of other computers. The *total storage space of all these computers* increases thereby taking on the extra data. It's immediately apparent that this is much more cost-effective - **scalability**. ‚úîÔ∏è\n",
    "\n",
    "![BusinessUrl](https://media.giphy.com/media/RHIYhjyA2R8IibyqPU/giphy.gif \"business\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Righty-ho, I want a NoSQL database, what are my choices? üôãüèæ\n",
    "\n",
    "For the purposes of this demonstration, we will use **mongoDB Atlas** as it provides a free, cloud-based 512MB cluster for us to experiment with.\n",
    "\n",
    "It is also the leading document (NoSQL) database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get our language right first, eh? üíÅüèΩ\n",
    "As No-SQL is a tad different to SQL, then the language to describe it are different too. Fret not though, there are analogues of SQL terminology which maps to No-SQL terminology!\n",
    "\n",
    "| SQL Term | No-SQL Term |\n",
    "| --- | --- | \n",
    "| Server | Cluster |\n",
    "| Database | Database |\n",
    "| Schema | Schema |\n",
    "| Table | Collection |\n",
    "| Row | Document |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Practical\n",
    "In this section, we will connect, query and download and import data from Kaggle to our MongoDB cluster.\n",
    "\n",
    "References are below:\n",
    "- [MongoDB naming convention](https://stackoverflow.com/questions/5916080/what-are-naming-conventions-for-mongodb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you say free? How do I create a NoSQL cluster then?! üí∏ü§ë\n",
    "To set-up your own free (up to 512MB of data) NoSQL cluster, please sign up for an account on the [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All well and good creating one, but how do I connect to my MongoDB cluster? üïµÔ∏è‚Äç‚ôÄÔ∏è\n",
    "To connect to your newly-created MongoDB cluster, will follow the below steps which are somewhat covered on the official [MongoDB Atlas documentation](https://docs.atlas.mongodb.com/driver-connection/) and blogs like this [one](https://code.tutsplus.com/tutorials/create-a-database-cluster-in-the-cloud-with-mongodb-atlas--cms-31840).\n",
    "1. On the MongoDB Atlas dashboard, create user group that has **READ-WRITE** access to the cluster.\n",
    "1. Create a JSON file and store these credentials in there so not every Tom, Dick and Harry can do stuff on your cluster.\n",
    "    + Including destructively destroy your data (*gasps*)! üî•üî•üî•üî•üî•\n",
    "1. In Python, import this JSON file.\n",
    "1. On the MongoDB Atlas dashboard, obtain the connection string for your cluster.\n",
    "1. In Python, feed the user credentials from your JSON file into your connection string.\n",
    "1. In Python, use this connection string to connect to your MongoDB cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials for connecting to MongoDB server\n",
    "with open(\"user_credentials.json\") as file_json:\n",
    "    data_credentials = json.load(file_json)\n",
    "print(data_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection string\n",
    "connect_user = data_credentials[\"user_group\"]\n",
    "connect_password = data_credentials[\"user_password\"]\n",
    " # obtain full connection string from MongoDB Atlas server dashboard\n",
    "connect_string = \"mongodb://\" + connect_user + \":\" + connect_password + \"@cluster-open-shard-00-00-kzzlc.mongodb.net:27017,cluster-open-shard-00-01-kzzlc.mongodb.net:27017,cluster-open-shard-00-02-kzzlc.mongodb.net:27017/test?replicaSet=cluster-open-shard-0&authSource=admin&ssl=true\"\n",
    "\n",
    "class Connect(object):\n",
    "    @staticmethod\n",
    "    def get_connection():\n",
    "        return MongoClient(connect_string)\n",
    "    \n",
    "# Call class just created to connect to MongoDB\n",
    "client = Connect.get_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Woop! Can I see what's already inside the cluster? üíÉüèº\n",
    "Let's have a look at the pre-existing **library** database as a quick check to see we are connected to the right cluster!\n",
    "\n",
    "*Whilst we outline how to import data into your cluster below, we also demonstrate how we have imported this data via the command shell in Javascript. Code is available [here](https://github.com/avisionh/Training-Distributed-Systems/blob/master/exercise_mongodb/db_insert_data.js)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_folder = path_folder + \"/data\"\n",
    "# Upload course_description JSON to compare with MongoDB contents\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move file to subfolder\n",
    "os.rename(\"exercise_json_course_description.json\", \"data/exercise_json_course_description.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 'library' database\n",
    "db = client.library\n",
    "\n",
    "# Retrieve all documents in 'authors' collection within the 'library' database\n",
    "cursor = db.authors.find({})\n",
    "for authors in cursor:\n",
    "     pprint(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool! Can I query this data now? ‚ùÑÔ∏è\n",
    "Good question! It's one thing viewing and storing data in our NoSQL database, but this isn't of much use until we extract and format it!\n",
    "\n",
    "Further reading:\n",
    " - [Python and mongoDB](https://www.mongodb.com/blog/post/getting-started-with-python-and-mongodb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering\n",
    "Say you are interested in all the documents of authors whose year of birth are later than 1941. You will do the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_filter = db['authors'].find_one({ \"yob\": {\"$gt\": 1941} })\n",
    "print(query_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of occurences\n",
    "Suppose you want to count the number of authors whose year of birth are later than 1941. You will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_count = db['authors'].count_documents({ \"yob\": {\"$gte\": 1941} })\n",
    "print(query_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation functions - grouping\n",
    "Suppose you want to sum the occurence of each year of birth, *yob*, in in your collection.\n",
    "\n",
    "You will make use of [mongoDB's aggregation framework](https://docs.mongodb.com/manual/aggregation/?&_ga=2.231777180.2063207926.1561451039-575593298.1559812962#aggregation-pipeline) to structure and process your query.\n",
    "\n",
    "Aggregation pipelines are defined as an array of different operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sum = db['authors'].aggregate([\n",
    "    # group data first\n",
    "    { \"$group\":\n",
    "        { \"_id\": \"$yob\",\n",
    "        \"count\": { \"$sum\":1}\n",
    "        }\n",
    "    },\n",
    "    # sort data\n",
    "    { \"$sort\": {\"_id\":1} }\n",
    "])\n",
    "\n",
    "# Print output\n",
    "for document in query_sum:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation functions - 'joining'\n",
    "Suppose now you want to look across the below collections:\n",
    "- authors\n",
    "- publishers\n",
    "- books\n",
    "To find the title(s) of the book(s) in the 'library' collection that has `Afred V. Aho` as an author.\n",
    "\n",
    "In SQL-speak, you are joining several tables and then filtering on the joined table. However, in a document-based database such as mongoDB, we don't have the concept of a join. Instead, we embed several **collections** (tables) and filter on the embedded table. üß†\n",
    "\n",
    "You will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_join = db['books'].aggregate([\n",
    "    # embed 'authors' collection into 'books' collection to filter on 'Alfred V. Aho' in '_id' field\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"authors\",\n",
    "            \"localField\": \"author\",\n",
    "            \"foreignField\": \"_id\",\n",
    "            \"as\": \"authorDetails\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$match\": {\"authorDetails._id\": \"AhoAV\"}\n",
    "        \n",
    "    }\n",
    "])\n",
    "\n",
    "# Print output\n",
    "for document in query_join:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I want to get my hands dirty now! How can I import data? üßó‚Äç‚ôÇÔ∏è\n",
    "Of course! We will investigate importing two different sets of data:\n",
    " - English Premier League football data ‚öΩÔ∏è\n",
    " - Countries, currencies, capital cities etc. data üéå\n",
    " \n",
    "In each of the data sets, there are subtle differences such that the way it appears as documents in your own MongoDB cluster is distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data 1: English Premier League ‚öΩÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Retrive data from Kaggle API and put in relevant folder\n",
    "kaggle.api.dataset_download_files('adithyarganesh/english-premier-league-player-data-20182019', \n",
    "                                  path = path_folder,\n",
    "                                  unzip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check it has been downloaded\n",
    "os.listdir(path_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Import EPL data into Python\n",
    "with open(path_folder + '/fpl_data_2018_2019.json') as data_football:\n",
    "    file_data = json.load(data_football)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. 'Create' 'footballDB' database\n",
    "#   note: not actually creating these in mongoDB yet\n",
    "db = client['footballDB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Import data into 'england' collection\n",
    "db['england'].insert_one(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then view the contents on the MongoDB Atlas dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data 2: Countries üéå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Country data for constructing links between country data\n",
    "kaggle.api.dataset_download_files('timoboz/country-data',\n",
    "                                  path = path_folder,\n",
    "                                  unzip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check it has been downloaded\n",
    "os.remove(\"data/exercise_json_course_description.json\")\n",
    "os.remove(\"data/fpl_data_2018_2019.json\")\n",
    "os.listdir(path_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Import country json file into Python session\n",
    "data_dicts = []\n",
    "for file in os.listdir(path_folder):\n",
    "    full_filename = \"%s/%s\" % (path_folder, file)\n",
    "    with open(full_filename,'r') as fi:\n",
    "        dict = json.load(fi)\n",
    "        data_dicts.append(dict)\n",
    "        \n",
    "del dict; del file; del full_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell is deprecated because cannot guarantee ordering of json files in our `data_dicts` dictionary object. Instead, manually remove the *\"exercise_json_course_description.json\"* and *\"fpl_data_2018_2019.json\"* files from **data** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Remove 4th and 5th list items since they are already in database\n",
    "# note: use pop() instead of del() because want to return the dict item\n",
    "#        being removed and store in a list\n",
    "# note: perform same operation 2 times so want to do a loop\n",
    "#        but don't want to store iterator so use '_' instead\n",
    "# reference: https://stackoverflow.com/a/2970808\n",
    "data_remove = [] \n",
    "for _ in range(2):\n",
    "    data_remove.append(data_dicts.pop(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Create database and collections for new data\n",
    "# note: these are not created in mongoDB yet,\n",
    "#        only exist in Python.\n",
    "#       when import into mongoDB, only then do we create\n",
    "#        the database and collections.\n",
    "db = client['countryDB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target:** Efficiently import several files in one-go and automated\n",
    "\n",
    "Idea here is to use a loop to iterate the MongoDB import for each collection:\n",
    "    \n",
    "    `db.<name>.insert_many(data_dicts[i])`\n",
    "    \n",
    "as have different variable names,\n",
    "need object which can be called from to use as variable names. e.g. collection_<name>\n",
    "is possible by using dictionary keys; not possible with lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two lists so can create a dictionary,\n",
    "# which is needed to use elements as variable names\n",
    "# so can loop over for each dictionary in data_dicts\n",
    "# to import into mongoDB\n",
    "collections = ['capitals','continents','currencies','isothree','names','phonecodes']\n",
    "# No need for this anymore since using db[key] instead!\n",
    "# redundant: db_collections = ['db.' + element for element in collections]\n",
    "dict_collections = {k:v for k, v in zip(collections, data_dicts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing into MongoDB now**\n",
    "\n",
    "Principle for how this nested loop works is\n",
    "   1. initialise outer loop\n",
    "   1. for each key-value pair in dict_collections, *e.g. 'capitals': {'AD':'Andorra la Vella', 'AE':'Abu Dhabi',...}*\n",
    "   1. create an empty lisbt object, list_dict, so we can append sub-key:sub-value objects in (wrapping them with '{}') *e.g. [{'AD':'Andorra la Vella'},{'AE':'Abu Dhabi'},...]*.\n",
    "       purpose of storing as list object is so can use `insert_many` function\n",
    "        to import more than one document into mongoDB\n",
    "   1. intialise sub-loop\n",
    "   1. for each (sub-)key-(sub-)value pair in the 'value' field of dict_collections, e.g. {'AD':'Andorra la Vella', 'AE':'Abu Dhabi',...}\n",
    "   1. wrap it within curly braces to turn each pair into a dict, and add as a new object within list_dict where aim was to get len(list_dict) = (sub-)key-(sub-)value pair in the 'value' field of dict_collections\n",
    "   1. back to outer loop\n",
    "   1. insert the list of dictionaries into mongoDB, e.g. `insert_many(list_dict)` under collection specified by the key field in data_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, value in dict_collections.items():\n",
    "    list_dict = []\n",
    "    for key_sub, value_sub in value.items():\n",
    "        list_dict.append({key_sub: value_sub})\n",
    "    db[key].insert_many(list_dict)\n",
    "\n",
    "del key; del value; del key_sub; del value_sub; del list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clear Workspace\n",
    "In this section, we will clear our workspace so that the script can be re-run and tested.\n",
    "\n",
    "### I'm a bit OCD, can we keep things tidy? üëª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erase files in data folder for next time\n",
    "for root, dirs, files in os.walk(path_folder, topdown = False):\n",
    "    for name in files:\n",
    "        os.remove(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erase collection for next time\n",
    "db[\"england\"].drop()\n",
    "for key in dict_collections.items():\n",
    "    db[key].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection for best practice\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Notes\n",
    "- Emojis copied and pasted into Jupyter notebook from [getemoji](http://getemoji.com/) üíÜüèªüíÖüèª\n",
    "\n",
    "![ShadesUrl](https://media.giphy.com/media/KXY5lB8yOarLy/giphy.gif \"shades\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "233px",
    "left": "1776.23px",
    "right": "20px",
    "top": "120px",
    "width": "341px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
